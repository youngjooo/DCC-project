{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "photos_face_shifter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngjooo/DCC-project/blob/main/photos_face_shifter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqAZmW7JXZ8P"
      },
      "source": [
        "# Clone code and download weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAhH90gNTKho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9139326a-3e02-4871-b574-c205a1c914a3"
      },
      "source": [
        "# clone model\n",
        "!git clone https://github.com/richarduuz/Research_Project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Research_Project'...\n",
            "remote: Enumerating objects: 418, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 418 (delta 4), reused 0 (delta 0), pack-reused 409\u001b[K\n",
            "Receiving objects: 100% (418/418), 43.76 MiB | 24.47 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnxf34beVy9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b053055f-a266-4b29-ab4a-bd0837b656a5"
      },
      "source": [
        "# download weights\n",
        "%cd /content/Research_Project/ModelC/face_modules/\n",
        "!gdown https://drive.google.com/uc?id=15nZSJ2bAT3m-iCBqP3N_9gld5_EGv4kp\n",
        "%cd /content/Research_Project/ModelC/saved_models/\n",
        "!gdown https://drive.google.com/uc?id=1iANX7oJoXCEECNzBEW1xOpac2tDOKeu9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Research_Project/ModelC/face_modules\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15nZSJ2bAT3m-iCBqP3N_9gld5_EGv4kp\n",
            "To: /content/Research_Project/ModelC/face_modules/model_ir_se50.pth\n",
            "175MB [00:01, 89.1MB/s]\n",
            "/content/Research_Project/ModelC/saved_models\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iANX7oJoXCEECNzBEW1xOpac2tDOKeu9\n",
            "To: /content/Research_Project/ModelC/saved_models/G_latest.pth\n",
            "737MB [00:05, 129MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-sG1Hd2XozG"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x0F0vw1YSFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde5d9c9-ae83-4d8a-fd90-880b5b280dbd"
      },
      "source": [
        "%cd /content/Research_Project/ModelC/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Research_Project/ModelC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGUfLUjNXoKo"
      },
      "source": [
        "import sys\n",
        "import torch\n",
        "sys.path.append('./face_modules/')\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from face_modules.model import Backbone, Arcface, MobileFaceNet, Am_softmax, l2_norm\n",
        "from network.AEI_Net import *\n",
        "from face_modules.mtcnn import *\n",
        "import cv2\n",
        "import PIL.Image as Image\n",
        "import numpy as np\n",
        "import configparser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0fe6jkSYe4u"
      },
      "source": [
        "detector = MTCNN()\n",
        "device = torch.device('cuda')\n",
        "G = AEI_Net(c_id=512)\n",
        "G.eval()\n",
        "G.load_state_dict(torch.load('./saved_models/G_latest.pth', map_location=torch.device('cpu')))\n",
        "G = G.cuda()\n",
        "\n",
        "arcface = Backbone(50, 0.6, 'ir_se').to(device)\n",
        "arcface.eval()\n",
        "arcface.load_state_dict(torch.load('./face_modules/model_ir_se50.pth', map_location=device), strict=False)\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CupgiblbU6KR"
      },
      "source": [
        "# Image Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aArVcgj8USzd"
      },
      "source": [
        "## Upload source image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK6ERNWTUOZJ"
      },
      "source": [
        "from google.colab import files\n",
        "source_img = files.upload()\n",
        "source_img_name = list(source_img.keys())[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HntTfyGCUyS-"
      },
      "source": [
        "## Upload target image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6TLv226U28G"
      },
      "source": [
        "from google.colab import files\n",
        "target_img = files.upload()\n",
        "target_img_name = list(target_img.keys())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMYJ0HM5VcM9"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOBmjCupVfAn"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "Xs_raw = cv2.imread(source_img_name)\n",
        "try:\n",
        "    Xs = detector.align(Image.fromarray(Xs_raw[:, :, ::-1]), crop_size=(256, 256))\n",
        "except Exception as e:\n",
        "    print('the source image is wrong, please change the image')\n",
        "Xs_raw = np.array(Xs)[:, :, ::-1]\n",
        "Xs = test_transform(Xs)\n",
        "Xs = Xs.unsqueeze(0).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeds = arcface(F.interpolate(Xs[:, :, 19:237, 19:237], (112, 112), mode='bilinear', align_corners=True))\n",
        "\n",
        "Xt_raw = cv2.imread(target_img_name)\n",
        "try:\n",
        "    Xt, trans_inv = detector.align(Image.fromarray(Xt_raw[:, :, ::-1]), crop_size=(256, 256), return_trans_inv=True)\n",
        "except Exception as e:\n",
        "    print('the target image is wrong, please change the image')\n",
        "Xt_raw = Xt_raw.astype(np.float)/255.0\n",
        "Xt = test_transform(Xt)\n",
        "Xt = Xt.unsqueeze(0).cuda()\n",
        "\n",
        "mask = np.zeros([256, 256], dtype=np.float)\n",
        "for i in range(256):\n",
        "    for j in range(256):\n",
        "        dist = np.sqrt((i-128)**2 + (j-128)**2)/128\n",
        "        dist = np.minimum(dist, 1)\n",
        "        mask[i, j] = 1-dist\n",
        "mask = cv2.dilate(mask, None, iterations=20)\n",
        "\n",
        "with torch.no_grad():\n",
        "    Yt, _ = G(Xt, embeds)\n",
        "    Yt = Yt.squeeze().detach().cpu().numpy().transpose([1, 2, 0])*0.5 + 0.5\n",
        "    Yt = Yt[:, :, ::-1]\n",
        "    Yt_trans_inv = cv2.warpAffine(Yt, trans_inv, (np.size(Xt_raw, 1), np.size(Xt_raw, 0)), borderValue=(0, 0, 0))\n",
        "    mask_ = cv2.warpAffine(mask,trans_inv, (np.size(Xt_raw, 1), np.size(Xt_raw, 0)), borderValue=(0, 0, 0))\n",
        "    mask_ = np.expand_dims(mask_, 2)\n",
        "    Yt_trans_inv = mask_*Yt_trans_inv + (1-mask_)*Xt_raw\n",
        "    cv2_imshow(Yt_trans_inv*255)\n",
        "    cv2.imwrite('./result.jpg',Yt_trans_inv*255)\n",
        "    print(\"the result image has been saved\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rItWDdHN-9Ux"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
